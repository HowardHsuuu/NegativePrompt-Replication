{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there NaN values:  False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>pnum</th>\n",
       "      <th>iteration</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word_in_context</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>word_in_context</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>word_in_context</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>word_in_context</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>word_in_context</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              task  pnum  iteration  score\n",
       "0  word_in_context     0          1   0.54\n",
       "1  word_in_context     0          2   0.52\n",
       "2  word_in_context     0          3   0.48\n",
       "3  word_in_context     0          4   0.55\n",
       "4  word_in_context     0          5   0.54"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"mistral_instruction_induction_no_shots.csv\", na_values=np.nan)\n",
    "print(\"Are there NaN values: \", df.isna().any().sum() is True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pnum==0\n",
      "                             mean       std\n",
      "task                                       \n",
      "active_to_passive        0.310000  0.018708\n",
      "antonyms                 0.402000  0.043243\n",
      "cause_and_effect         0.520000  0.126491\n",
      "common_concept           0.009804  0.011172\n",
      "diff                     0.678000  0.014832\n",
      "first_word_letter        0.982000  0.008367\n",
      "informal_to_formal       0.331919  0.033609\n",
      "larger_animal            0.604000  0.021909\n",
      "letters_list             0.056000  0.008944\n",
      "negation                 0.316000  0.008944\n",
      "num_to_verbal            0.636000  0.008944\n",
      "orthography_starts_with  0.066000  0.023022\n",
      "rhymes                   0.082000  0.021679\n",
      "second_word_letter       0.222000  0.020494\n",
      "sentence_similarity      0.304000  0.043932\n",
      "sentiment                0.872000  0.021679\n",
      "singular_to_plural       0.738000  0.025884\n",
      "sum                      0.658000  0.029496\n",
      "synonyms                 0.084000  0.016733\n",
      "taxonomy_animal          0.054000  0.018166\n",
      "translation_en-de        0.206000  0.013416\n",
      "translation_en-es        0.366000  0.011402\n",
      "translation_en-fr        0.394000  0.046152\n",
      "word_in_context          0.526000  0.027928\n",
      "pnum>0\n",
      "                         mean       std\n",
      "task              pnum                 \n",
      "active_to_passive 1     0.378  0.049699\n",
      "                  2     0.376  0.028810\n",
      "                  3     0.412  0.021679\n",
      "                  4     0.230  0.029155\n",
      "                  5     0.226  0.038471\n",
      "...                       ...       ...\n",
      "word_in_context   6     0.576  0.021909\n",
      "                  7     0.514  0.030496\n",
      "                  8     0.418  0.069065\n",
      "                  9     0.512  0.056745\n",
      "                  10    0.502  0.032711\n",
      "\n",
      "[240 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Base case (pnum=0) statistics\n",
    "base_case_stats = df[df['pnum'] == 0].groupby('task')['score'].agg(['mean', 'std'])\n",
    "\n",
    "# Compute the same statistics for each other pnum (pnum > 0)\n",
    "modified_stats = df[df['pnum'] > 0].groupby(['task', 'pnum'])['score'].agg(['mean', 'std'])\n",
    "\n",
    "# Display the statistics for comparison\n",
    "print(\"pnum==0\")\n",
    "print(base_case_stats)\n",
    "\n",
    "print(\"pnum>0\")\n",
    "print(modified_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>pnum</th>\n",
       "      <th>iteration</th>\n",
       "      <th>score</th>\n",
       "      <th>base_score</th>\n",
       "      <th>score_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word_in_context</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>word_in_context</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>word_in_context</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>word_in_context</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>word_in_context</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>word_in_context</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>word_in_context</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>word_in_context</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>word_in_context</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>word_in_context</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              task  pnum  iteration  score  base_score  score_diff\n",
       "0  word_in_context     1          1   0.52        0.54       -0.02\n",
       "1  word_in_context     2          1   0.50        0.54       -0.04\n",
       "2  word_in_context     3          1   0.51        0.54       -0.03\n",
       "3  word_in_context     4          1   0.46        0.54       -0.08\n",
       "4  word_in_context     5          1   0.55        0.54        0.01\n",
       "5  word_in_context     6          1   0.58        0.54        0.04\n",
       "6  word_in_context     7          1   0.49        0.54       -0.05\n",
       "7  word_in_context     8          1   0.43        0.54       -0.11\n",
       "8  word_in_context     9          1   0.47        0.54       -0.07\n",
       "9  word_in_context    10          1   0.49        0.54       -0.05"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the differences between base case (pnum=0) and modified prompts (pnum > 0) for each task\n",
    "# Merge base case with modified stats to compute differences\n",
    "base_scores = df[df['pnum'] == 0][['task', 'iteration', 'score']].rename(columns={'score': 'base_score'})\n",
    "\n",
    "# Merge with original dataframe (pnum > 0)\n",
    "diff_df = pd.merge(df[df['pnum'] > 0], base_scores, on=['task', 'iteration'], how='inner')\n",
    "\n",
    "# Calculate the difference between modified and base scores\n",
    "diff_df['score_diff'] = diff_df['score'] - diff_df['base_score']\n",
    "\n",
    "# Display the first few rows to check the differences\n",
    "diff_df[['task', 'pnum', 'iteration', 'score', 'base_score', 'score_diff']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 112 non-significant pnum-tasks combinations on a total of 240 (46.67%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>pnum</th>\n",
       "      <th>t_stat</th>\n",
       "      <th>p_value</th>\n",
       "      <th>power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>orthography_starts_with</td>\n",
       "      <td>8</td>\n",
       "      <td>2.745223e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>larger_animal</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>sentence_similarity</td>\n",
       "      <td>4</td>\n",
       "      <td>-8.347839e-02</td>\n",
       "      <td>0.937482</td>\n",
       "      <td>0.053023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cause_and_effect</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.829464e-02</td>\n",
       "      <td>0.926427</td>\n",
       "      <td>0.053762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>second_word_letter</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.034175e-01</td>\n",
       "      <td>0.922609</td>\n",
       "      <td>0.054188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>taxonomy_animal</td>\n",
       "      <td>8</td>\n",
       "      <td>1.727737e-01</td>\n",
       "      <td>0.871219</td>\n",
       "      <td>0.058162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>sentence_similarity</td>\n",
       "      <td>6</td>\n",
       "      <td>2.857143e-01</td>\n",
       "      <td>0.789282</td>\n",
       "      <td>0.059913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>common_concept</td>\n",
       "      <td>6</td>\n",
       "      <td>2.048393e-01</td>\n",
       "      <td>0.847699</td>\n",
       "      <td>0.062501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>translation_en-fr</td>\n",
       "      <td>6</td>\n",
       "      <td>-2.242305e-01</td>\n",
       "      <td>0.833566</td>\n",
       "      <td>0.063969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>sentence_similarity</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.544567e-01</td>\n",
       "      <td>0.811689</td>\n",
       "      <td>0.069207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>rhymes</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.215395e-01</td>\n",
       "      <td>0.835523</td>\n",
       "      <td>0.077517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>sentence_similarity</td>\n",
       "      <td>7</td>\n",
       "      <td>2.211629e-01</td>\n",
       "      <td>0.835797</td>\n",
       "      <td>0.078366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>orthography_starts_with</td>\n",
       "      <td>5</td>\n",
       "      <td>-3.429972e-01</td>\n",
       "      <td>0.748868</td>\n",
       "      <td>0.080027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>antonyms</td>\n",
       "      <td>6</td>\n",
       "      <td>-2.324953e-01</td>\n",
       "      <td>0.827565</td>\n",
       "      <td>0.081444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>antonyms</td>\n",
       "      <td>5</td>\n",
       "      <td>-2.694080e-01</td>\n",
       "      <td>0.800942</td>\n",
       "      <td>0.083000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        task  pnum        t_stat   p_value     power\n",
       "117  orthography_starts_with     8  2.745223e-16  1.000000  0.050000\n",
       "74             larger_animal     5  0.000000e+00  1.000000  0.050000\n",
       "143      sentence_similarity     4 -8.347839e-02  0.937482  0.053023\n",
       "20          cause_and_effect     1 -9.829464e-02  0.926427  0.053762\n",
       "130       second_word_letter     1 -1.034175e-01  0.922609  0.054188\n",
       "197          taxonomy_animal     8  1.727737e-01  0.871219  0.058162\n",
       "145      sentence_similarity     6  2.857143e-01  0.789282  0.059913\n",
       "35            common_concept     6  2.048393e-01  0.847699  0.062501\n",
       "225        translation_en-fr     6 -2.242305e-01  0.833566  0.063969\n",
       "142      sentence_similarity     3 -2.544567e-01  0.811689  0.069207\n",
       "123                   rhymes     4 -2.215395e-01  0.835523  0.077517\n",
       "146      sentence_similarity     7  2.211629e-01  0.835797  0.078366\n",
       "114  orthography_starts_with     5 -3.429972e-01  0.748868  0.080027\n",
       "15                  antonyms     6 -2.324953e-01  0.827565  0.081444\n",
       "14                  antonyms     5 -2.694080e-01  0.800942  0.083000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "from statsmodels.stats.power import TTestPower\n",
    "\n",
    "# Cohen's d for independent samples\n",
    "def cohen_d(x, y):\n",
    "    # Mean difference between the two groups\n",
    "    mean_diff = np.mean(x) - np.mean(y)\n",
    "    \n",
    "    # Pooled standard deviation\n",
    "    n_x, n_y = len(x), len(y)\n",
    "    pooled_std = np.sqrt(((n_x - 1) * np.std(x, ddof=1) ** 2 + (n_y - 1) * np.std(y, ddof=1) ** 2) / (n_x + n_y - 2))\n",
    "    \n",
    "    # Cohen's d\n",
    "    return mean_diff / pooled_std\n",
    "\n",
    "# Statistical power calculation\n",
    "def statistical_power(base_scores: list, modified_scores: list, alpha=0.05):\n",
    "    # Calculate Cohen's d\n",
    "    d = cohen_d(modified_scores, base_scores)\n",
    "\n",
    "    # power analysis\n",
    "    power_analysis = TTestPower()\n",
    "    sample_size = len(base_scores) + len(modified_scores)  # Total sample size for both groups\n",
    "    return power_analysis.power(effect_size=d, nobs=sample_size, alpha=alpha, alternative='two-sided')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "# threshold for significance\n",
    "alpha = 0.05\n",
    "\n",
    "# Group by task and pnum to perform the paired t-test\n",
    "for (task, pnum), group in diff_df.groupby(['task', 'pnum']):\n",
    "    base_scores = group['base_score']\n",
    "    modified_scores = group['score']\n",
    "    \n",
    "    # paired t-test\n",
    "    t_stat, p_value = ttest_rel(modified_scores, base_scores)\n",
    "\n",
    "    # power analysis\n",
    "    power = statistical_power(base_scores=base_scores, modified_scores=modified_scores)\n",
    "    \n",
    "    # Store the results\n",
    "    results.append({'task': task, 'pnum': pnum, 't_stat': t_stat, 'p_value': p_value, 'power': power})\n",
    "\n",
    "# Convert results into a dataframe for easier viewing\n",
    "ttest_results_df = pd.DataFrame(results)\n",
    "\n",
    "non_significant_df = ttest_results_df[ttest_results_df['p_value'] > alpha]\n",
    "print(f\"There are {len(non_significant_df)} non-significant pnum-tasks combinations on a total of {len(ttest_results_df)} ({100*len(non_significant_df)/len(ttest_results_df):.2f}%)\")\n",
    "\n",
    "# Display the results\n",
    "ttest_results_df.sort_values(by='power', ascending=True).head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_mean': 0.3924051348737096,\n",
      " 'base_std': 0.2728550474864339,\n",
      " 'modified_mean': 0.33392197945239094,\n",
      " 'modified_std': 0.2625328608703769,\n",
      " 'p_value': 0.02623052485174041,\n",
      " 'power': 1.0,\n",
      " 'significant': True,\n",
      " 't_stat': 2.246277411265254}\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "from pprint import pprint\n",
    "\n",
    "# 1. Calculate mean and standard deviation for all pnum == 0 (base case)\n",
    "base_scores = df[df['pnum'] == 0]['score']\n",
    "base_mean = base_scores.mean()\n",
    "base_std = base_scores.std()\n",
    "\n",
    "# 2. Calculate mean and standard deviation for all pnum > 0 (modified cases)\n",
    "modified_scores = df[df['pnum'] > 0]['score']\n",
    "modified_mean = modified_scores.mean()\n",
    "modified_std = modified_scores.std()\n",
    "\n",
    "# 3. Perform independent two-sample t-test\n",
    "t_stat, p_value = ttest_ind(base_scores, modified_scores, equal_var=False)  # Welch's t-test\n",
    "power = statistical_power(base_scores=base_scores, modified_scores=modified_scores)\n",
    "\n",
    "# 4. Check significance at alpha = 0.05\n",
    "alpha = 0.05\n",
    "significant = p_value < alpha\n",
    "\n",
    "# 5. Output results\n",
    "results = {\n",
    "    'base_mean': base_mean,\n",
    "    'base_std': base_std,\n",
    "    'modified_mean': modified_mean,\n",
    "    'modified_std': modified_std,\n",
    "    't_stat': t_stat,\n",
    "    'p_value': p_value,\n",
    "    'significant': significant,\n",
    "    'power': power\n",
    "}\n",
    "\n",
    "# Display the results\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_mean': 0.3924051348737096,\n",
      " 'base_std': 0.2728550474864339,\n",
      " 'best_pnum': 2,\n",
      " 'best_pnum_mean': 0.36823392539631905,\n",
      " 'best_pnum_std': 0.27792442756518954,\n",
      " 'p_value': 0.4972655741214477,\n",
      " 'power': 0.2728027424168394,\n",
      " 'significant': False,\n",
      " 't_stat': 0.6798417953260264}\n"
     ]
    }
   ],
   "source": [
    "# 1. Calculate mean scores for each pnum > 0\n",
    "pnum_means = df[df['pnum'] > 0].groupby('pnum')['score'].mean()\n",
    "\n",
    "# 2. Identify the pnum with the highest mean score\n",
    "best_pnum = pnum_means.idxmax()\n",
    "\n",
    "# 3. Calculate mean and std for pnum == 0 (base case)\n",
    "base_scores = df[df['pnum'] == 0]['score']\n",
    "base_mean = base_scores.mean()\n",
    "base_std = base_scores.std()\n",
    "\n",
    "# 4. Calculate mean and std for the pnum with the highest mean score\n",
    "best_pnum_scores = df[df['pnum'] == best_pnum]['score']\n",
    "best_pnum_mean = best_pnum_scores.mean()\n",
    "best_pnum_std = best_pnum_scores.std()\n",
    "\n",
    "# 5. Perform independent two-sample t-test between pnum == 0 and the best pnum\n",
    "t_stat, p_value = ttest_ind(base_scores, best_pnum_scores, equal_var=False)  # Welch's t-test\n",
    "power = statistical_power(base_scores=base_scores, modified_scores=best_pnum_scores)\n",
    "\n",
    "# 6. Check significance at alpha = 0.05\n",
    "alpha = 0.05\n",
    "significant = p_value < alpha\n",
    "\n",
    "# 7. Output results\n",
    "results = {\n",
    "    'base_mean': base_mean,\n",
    "    'base_std': base_std,\n",
    "    'best_pnum': best_pnum,\n",
    "    'best_pnum_mean': best_pnum_mean,\n",
    "    'best_pnum_std': best_pnum_std,\n",
    "    't_stat': t_stat,\n",
    "    'p_value': p_value,\n",
    "    'significant': significant,\n",
    "    'power': power\n",
    "}\n",
    "\n",
    "# Display the results\n",
    "pprint(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 10.3",
   "language": "sage",
   "name": "SageMath-10.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
